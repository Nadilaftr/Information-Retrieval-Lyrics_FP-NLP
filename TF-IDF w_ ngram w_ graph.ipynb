{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14rgk9FRiO-UVb82jhVzrl0lFBaBxDj4k","timestamp":1734531140529},{"file_id":"13CNdOh5ljAUBYvLUR67hXsGUi_6uFx1j","timestamp":1734527059052},{"file_id":"1MarAXFCIA1Y_Z3SpG3Bg6j1OtSgGVNE3","timestamp":1734436990444},{"file_id":"1KeTeMxQdl4TDiHxaTFn9Z4BgnaRqWuzy","timestamp":1734237352881}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TF-IDF with ngram"],"metadata":{"id":"XEDsEEBlxJbL"}},{"cell_type":"markdown","source":["## Simple"],"metadata":{"id":"qrc74jNk2pn6"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import warnings\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from collections import defaultdict\n","import math\n","from sklearn.metrics.pairwise import cosine_similarity\n","warnings.filterwarnings(\"ignore\")\n","\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","\n","lyrics_df = pd.read_excel('/content/databersih (2).xlsx')\n","ground_truth_df = pd.read_excel('/content/GT UAS NLP_simple.xlsx')\n","\n","stemmer = PorterStemmer()\n","remove_words = [\"song\", \"with\", \"lyrics\", \"from\", \"the\", \"album\", \"released\", \"in\", \"before\", \"after\", \"since\", \"s\"]"],"metadata":{"id":"MQuxw9RABBvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734559275402,"user_tz":-420,"elapsed":14556,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"8896e066-d7c9-42f1-d3c5-ff79a06c0ea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"code","source":["def clean_lyrics(text):\n","    if isinstance(text, str):\n","        text = re.sub(r'\\[.*?\\]', '', text)\n","        text = re.sub(r'\\(.*?\\)', '', text)\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        text = re.sub(r'\\d+', '', text)\n","        text = text.lower()\n","        tokens = word_tokenize(text)\n","        tokens = [stemmer.stem(word) for word in tokens if word not in remove_words]\n","        text = ' '.join(tokens)\n","    return text\n","\n","lyrics_df['Processed_Lyrics'] = lyrics_df['lyrics'].apply(clean_lyrics)\n","\n","print(\"\\nCleaned Lyrics Sample:\")\n","print(lyrics_df[['title', 'Processed_Lyrics']].head())\n","\n","def parse_song_ids(song_id_entry):\n","    if pd.isnull(song_id_entry):\n","        return []\n","    if isinstance(song_id_entry, int):\n","        return [song_id_entry]\n","    if isinstance(song_id_entry, float) and np.isnan(song_id_entry):\n","        return []\n","    song_id_str = str(song_id_entry)\n","    return [int(id_.strip()) for id_ in song_id_str.split(',') if id_.strip().isdigit()]\n","\n","ground_truth_df['Relevant_Song_IDs'] = ground_truth_df['song_id'].apply(parse_song_ids)\n","ground_truth_df = ground_truth_df[ground_truth_df['Relevant_Song_IDs'].map(len) > 0].reset_index(drop=True)\n","print(\"\\nParsed Ground Truth:\")\n","print(ground_truth_df[['query', 'Relevant_Song_IDs', 'total']].head())\n","\n","def preprocess_query(query):\n","    if isinstance(query, str):\n","        query = re.sub(r'[^\\w\\s]', '', query)\n","        query = re.sub(r'\\d+', '', query)\n","        query = query.lower()\n","        tokens = word_tokenize(query)\n","        tokens = [stemmer.stem(word) for word in tokens if word not in remove_words]\n","        return ' '.join(tokens)\n","    return query\n","\n","ground_truth_df['Processed_Query'] = ground_truth_df['query'].apply(preprocess_query)"],"metadata":{"id":"Pk20uurQBAa4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734559317745,"user_tz":-420,"elapsed":42375,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"03dcb3a0-fa34-4527-9e3e-81935ac59ec1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cleaned Lyrics Sample:\n","                   title                                   Processed_Lyrics\n","0      Chasing Pavements  ive made up my mind dont need to think it over...\n","1          Cold Shoulder  you say it all my head and thing i think just ...\n","2         Hometown Glory  ive been walk same way as i did miss out crack...\n","3  Make You Feel My Love  when rain is blow your face and whole world is...\n","4                My Same  aye aye ayeay aye aye ayeay aye aye ayeay aye ...\n","\n","Parsed Ground Truth:\n","           query                                  Relevant_Song_IDs  total\n","0    love you in          [129, 1433, 1977, 2978, 3214, 3320, 4514]      7\n","1   home tonight  [561, 1019, 1455, 1671, 1686, 2428, 2434, 3126...     12\n","2  something new  [20, 484, 671, 783, 999, 1463, 2556, 2607, 264...     18\n","3     i held you                             [44, 1520, 4082, 4503]      4\n","4  stars tonight                                   [95, 2142, 3177]      3\n"]}]},{"cell_type":"code","source":["ground_truth_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"id":"MME4FS-PQIDj","executionInfo":{"status":"ok","timestamp":1734559337133,"user_tz":-420,"elapsed":474,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"52689515-0a21-4ff2-83ee-1d2daa028c9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 query                                            song_id  \\\n","0          love you in            129, 1433, 1977, 2978, 3214, 3320, 4514   \n","1         home tonight  561, 1019, 1455, 1671, 1686, 2428, 2434, 3126,...   \n","2        something new  20, 484, 671, 783, 999, 1463, 2556, 2607, 2642...   \n","3           i held you                               44, 1520, 4082, 4503   \n","4        stars tonight                                     95, 2142, 3177   \n","5        know about me       51, 1261, 1293, 2721, 2729, 3426, 3858, 4865   \n","6       stick together                                          227, 2325   \n","7            easy come                         239, 530, 2137, 3407, 4471   \n","8             haunt me        257, 723, 966, 1290, 1498, 2245, 2618, 3433   \n","9     i want your love                              542, 1036, 1992, 3811   \n","10          the ripper                     557, 642, 659, 857, 2804, 3350   \n","11         stupid shit  599, 689, 848, 1354, 1382, 2445, 3688, 4025, 4844   \n","12   all over the room                                          989, 3502   \n","13  don't wanna let go                                    174, 1733, 2579   \n","14    like a butterfly                                111, 614, 925, 2731   \n","15   when we first met                        993, 1018, 3023, 4293, 4534   \n","16    i cross my heart                             3638, 3948, 4523, 4685   \n","17       touch the sky  361, 800, 911, 1352, 1837, 3235, 3270, 3594, 4060   \n","18     are you willing                                795, 805, 895, 1156   \n","19     come right back              447, 578, 759, 1199, 2052, 2167, 2465   \n","20        suit and tie       361, 417, 1212, 3297, 3399, 3634, 3676, 4635   \n","21      that i breathe             187, 870, 2009, 2201, 3223, 3330, 3918   \n","22      stayed at home                                          515, 4381   \n","23   different enemies                                         4898, 4904   \n","24        get me wrong               406, 599, 713, 804, 1430, 2743, 4354   \n","\n","    total                                  Relevant_Song_IDs  \\\n","0       7          [129, 1433, 1977, 2978, 3214, 3320, 4514]   \n","1      12  [561, 1019, 1455, 1671, 1686, 2428, 2434, 3126...   \n","2      18  [20, 484, 671, 783, 999, 1463, 2556, 2607, 264...   \n","3       4                             [44, 1520, 4082, 4503]   \n","4       3                                   [95, 2142, 3177]   \n","5       8     [51, 1261, 1293, 2721, 2729, 3426, 3858, 4865]   \n","6       2                                        [227, 2325]   \n","7       5                       [239, 530, 2137, 3407, 4471]   \n","8       8      [257, 723, 966, 1290, 1498, 2245, 2618, 3433]   \n","9       4                            [542, 1036, 1992, 3811]   \n","10      6                   [557, 642, 659, 857, 2804, 3350]   \n","11      9  [599, 689, 848, 1354, 1382, 2445, 3688, 4025, ...   \n","12      2                                        [989, 3502]   \n","13      3                                  [174, 1733, 2579]   \n","14      4                              [111, 614, 925, 2731]   \n","15      5                      [993, 1018, 3023, 4293, 4534]   \n","16      4                           [3638, 3948, 4523, 4685]   \n","17      9  [361, 800, 911, 1352, 1837, 3235, 3270, 3594, ...   \n","18      4                              [795, 805, 895, 1156]   \n","19      7            [447, 578, 759, 1199, 2052, 2167, 2465]   \n","20      8     [361, 417, 1212, 3297, 3399, 3634, 3676, 4635]   \n","21      7           [187, 870, 2009, 2201, 3223, 3330, 3918]   \n","22      2                                        [515, 4381]   \n","23      2                                       [4898, 4904]   \n","24      7             [406, 599, 713, 804, 1430, 2743, 4354]   \n","\n","       Processed_Query  \n","0             love you  \n","1         home tonight  \n","2           someth new  \n","3           i held you  \n","4         star tonight  \n","5        know about me  \n","6         stick togeth  \n","7            easi come  \n","8             haunt me  \n","9     i want your love  \n","10              ripper  \n","11         stupid shit  \n","12       all over room  \n","13  dont wan na let go  \n","14    like a butterfli  \n","15   when we first met  \n","16    i cross my heart  \n","17           touch sky  \n","18        are you will  \n","19     come right back  \n","20        suit and tie  \n","21       that i breath  \n","22        stay at home  \n","23        differ enemi  \n","24        get me wrong  "],"text/html":["\n","  <div id=\"df-d103b6ee-ffcc-4ed7-80c9-340800ae5361\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>query</th>\n","      <th>song_id</th>\n","      <th>total</th>\n","      <th>Relevant_Song_IDs</th>\n","      <th>Processed_Query</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>love you in</td>\n","      <td>129, 1433, 1977, 2978, 3214, 3320, 4514</td>\n","      <td>7</td>\n","      <td>[129, 1433, 1977, 2978, 3214, 3320, 4514]</td>\n","      <td>love you</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>home tonight</td>\n","      <td>561, 1019, 1455, 1671, 1686, 2428, 2434, 3126,...</td>\n","      <td>12</td>\n","      <td>[561, 1019, 1455, 1671, 1686, 2428, 2434, 3126...</td>\n","      <td>home tonight</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>something new</td>\n","      <td>20, 484, 671, 783, 999, 1463, 2556, 2607, 2642...</td>\n","      <td>18</td>\n","      <td>[20, 484, 671, 783, 999, 1463, 2556, 2607, 264...</td>\n","      <td>someth new</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i held you</td>\n","      <td>44, 1520, 4082, 4503</td>\n","      <td>4</td>\n","      <td>[44, 1520, 4082, 4503]</td>\n","      <td>i held you</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>stars tonight</td>\n","      <td>95, 2142, 3177</td>\n","      <td>3</td>\n","      <td>[95, 2142, 3177]</td>\n","      <td>star tonight</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>know about me</td>\n","      <td>51, 1261, 1293, 2721, 2729, 3426, 3858, 4865</td>\n","      <td>8</td>\n","      <td>[51, 1261, 1293, 2721, 2729, 3426, 3858, 4865]</td>\n","      <td>know about me</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>stick together</td>\n","      <td>227, 2325</td>\n","      <td>2</td>\n","      <td>[227, 2325]</td>\n","      <td>stick togeth</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>easy come</td>\n","      <td>239, 530, 2137, 3407, 4471</td>\n","      <td>5</td>\n","      <td>[239, 530, 2137, 3407, 4471]</td>\n","      <td>easi come</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>haunt me</td>\n","      <td>257, 723, 966, 1290, 1498, 2245, 2618, 3433</td>\n","      <td>8</td>\n","      <td>[257, 723, 966, 1290, 1498, 2245, 2618, 3433]</td>\n","      <td>haunt me</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>i want your love</td>\n","      <td>542, 1036, 1992, 3811</td>\n","      <td>4</td>\n","      <td>[542, 1036, 1992, 3811]</td>\n","      <td>i want your love</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>the ripper</td>\n","      <td>557, 642, 659, 857, 2804, 3350</td>\n","      <td>6</td>\n","      <td>[557, 642, 659, 857, 2804, 3350]</td>\n","      <td>ripper</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>stupid shit</td>\n","      <td>599, 689, 848, 1354, 1382, 2445, 3688, 4025, 4844</td>\n","      <td>9</td>\n","      <td>[599, 689, 848, 1354, 1382, 2445, 3688, 4025, ...</td>\n","      <td>stupid shit</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>all over the room</td>\n","      <td>989, 3502</td>\n","      <td>2</td>\n","      <td>[989, 3502]</td>\n","      <td>all over room</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>don't wanna let go</td>\n","      <td>174, 1733, 2579</td>\n","      <td>3</td>\n","      <td>[174, 1733, 2579]</td>\n","      <td>dont wan na let go</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>like a butterfly</td>\n","      <td>111, 614, 925, 2731</td>\n","      <td>4</td>\n","      <td>[111, 614, 925, 2731]</td>\n","      <td>like a butterfli</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>when we first met</td>\n","      <td>993, 1018, 3023, 4293, 4534</td>\n","      <td>5</td>\n","      <td>[993, 1018, 3023, 4293, 4534]</td>\n","      <td>when we first met</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>i cross my heart</td>\n","      <td>3638, 3948, 4523, 4685</td>\n","      <td>4</td>\n","      <td>[3638, 3948, 4523, 4685]</td>\n","      <td>i cross my heart</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>touch the sky</td>\n","      <td>361, 800, 911, 1352, 1837, 3235, 3270, 3594, 4060</td>\n","      <td>9</td>\n","      <td>[361, 800, 911, 1352, 1837, 3235, 3270, 3594, ...</td>\n","      <td>touch sky</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>are you willing</td>\n","      <td>795, 805, 895, 1156</td>\n","      <td>4</td>\n","      <td>[795, 805, 895, 1156]</td>\n","      <td>are you will</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>come right back</td>\n","      <td>447, 578, 759, 1199, 2052, 2167, 2465</td>\n","      <td>7</td>\n","      <td>[447, 578, 759, 1199, 2052, 2167, 2465]</td>\n","      <td>come right back</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>suit and tie</td>\n","      <td>361, 417, 1212, 3297, 3399, 3634, 3676, 4635</td>\n","      <td>8</td>\n","      <td>[361, 417, 1212, 3297, 3399, 3634, 3676, 4635]</td>\n","      <td>suit and tie</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>that i breathe</td>\n","      <td>187, 870, 2009, 2201, 3223, 3330, 3918</td>\n","      <td>7</td>\n","      <td>[187, 870, 2009, 2201, 3223, 3330, 3918]</td>\n","      <td>that i breath</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>stayed at home</td>\n","      <td>515, 4381</td>\n","      <td>2</td>\n","      <td>[515, 4381]</td>\n","      <td>stay at home</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>different enemies</td>\n","      <td>4898, 4904</td>\n","      <td>2</td>\n","      <td>[4898, 4904]</td>\n","      <td>differ enemi</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>get me wrong</td>\n","      <td>406, 599, 713, 804, 1430, 2743, 4354</td>\n","      <td>7</td>\n","      <td>[406, 599, 713, 804, 1430, 2743, 4354]</td>\n","      <td>get me wrong</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d103b6ee-ffcc-4ed7-80c9-340800ae5361')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d103b6ee-ffcc-4ed7-80c9-340800ae5361 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d103b6ee-ffcc-4ed7-80c9-340800ae5361');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d45646dc-fcf8-4281-afa9-f54297753c39\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d45646dc-fcf8-4281-afa9-f54297753c39')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d45646dc-fcf8-4281-afa9-f54297753c39 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8c6802ba-ffb9-4ebf-bf76-de56bbb59de6\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ground_truth_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8c6802ba-ffb9-4ebf-bf76-de56bbb59de6 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('ground_truth_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"ground_truth_df","summary":"{\n  \"name\": \"ground_truth_df\",\n  \"rows\": 25,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"haunt me\",\n          \"i cross my heart\",\n          \"love you in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"song_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"257, 723, 966, 1290, 1498, 2245, 2618, 3433\",\n          \"3638, 3948, 4523, 4685\",\n          \"129, 1433, 1977, 2978, 3214, 3320, 4514\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 18,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          6,\n          12,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Relevant_Song_IDs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processed_Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"haunt me\",\n          \"i cross my heart\",\n          \"love you\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["lyrics_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"MkVvEM_rWxsP","executionInfo":{"status":"ok","timestamp":1734559341026,"user_tz":-420,"elapsed":412,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"e82b7526-5806-42a3-877b-f3f104176c0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        no        artist                             album  year  \\\n","0        1         Adele                                19  2008   \n","1        2         Adele                                19  2008   \n","2        3         Adele                                19  2008   \n","3        4         Adele                                19  2008   \n","4        5         Adele                                19  2008   \n","...    ...           ...                               ...   ...   \n","4907  4908  XXXTentacion                          The Fall  2014   \n","4908  4909  XXXTentacion                          The Fall  2014   \n","4909  4910  XXXTentacion                          The Fall  2014   \n","4910  4911  XXXTentacion  Willy Wonka Was a Child Murderer  2016   \n","4911  4912  XXXTentacion                               XXX  2014   \n","\n","                                 title  \\\n","0                    Chasing Pavements   \n","1                        Cold Shoulder   \n","2                       Hometown Glory   \n","3                Make You Feel My Love   \n","4                              My Same   \n","...                                ...   \n","4907                          The Fall   \n","4908                            ​ghost   \n","4909                       ​white girl   \n","4910  Willy Wonka Was a Child Murderer   \n","4911                 DUTTY (Freestyle)   \n","\n","                                                 lyrics  \\\n","0     I've made up my mind\\nDon't need to think it o...   \n","1     \\nYou say it's all in my head\\nAnd the things ...   \n","2     I've been walking in the same way as I did\\nMi...   \n","3     When the rain is blowing in your face\\nAnd the...   \n","4     Aye, aye, aye-aye\\nAye, aye, aye-aye\\nAye, aye...   \n","...                                                 ...   \n","4907  Fool's gold is a common man's trash\\nI've seen...   \n","4908  Shtuom ruo neewteb seid ecapS tuo dna ni revO\\...   \n","4909  Haha\\nYou know, gang, gang, bitch\\nXXX, pussy ...   \n","4910  Yeah\\nIt's all in my, it's all in my head\\nIt'...   \n","4911  Jah, Jah, Jah, ha-ha\\nSee me\\nHip-Hop's a jung...   \n","\n","                                       Processed_Lyrics  \n","0     ive made up my mind dont need to think it over...  \n","1     you say it all my head and thing i think just ...  \n","2     ive been walk same way as i did miss out crack...  \n","3     when rain is blow your face and whole world is...  \n","4     aye aye ayeay aye aye ayeay aye aye ayeay aye ...  \n","...                                                 ...  \n","4907  fool gold is a common man trash ive seen god i...  \n","4908  shtuom ruo neewteb seid ecap tuo dna ni revo m...  \n","4909  haha you know gang gang bitch xxx pussi boy fa...  \n","4910  yeah it all my it all my head it all my it all...  \n","4911  jah jah jah haha see me hiphop a jungl im deep...  \n","\n","[4912 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-168bc780-770f-43c9-9301-077d68809a87\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no</th>\n","      <th>artist</th>\n","      <th>album</th>\n","      <th>year</th>\n","      <th>title</th>\n","      <th>lyrics</th>\n","      <th>Processed_Lyrics</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Adele</td>\n","      <td>19</td>\n","      <td>2008</td>\n","      <td>Chasing Pavements</td>\n","      <td>I've made up my mind\\nDon't need to think it o...</td>\n","      <td>ive made up my mind dont need to think it over...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Adele</td>\n","      <td>19</td>\n","      <td>2008</td>\n","      <td>Cold Shoulder</td>\n","      <td>\\nYou say it's all in my head\\nAnd the things ...</td>\n","      <td>you say it all my head and thing i think just ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Adele</td>\n","      <td>19</td>\n","      <td>2008</td>\n","      <td>Hometown Glory</td>\n","      <td>I've been walking in the same way as I did\\nMi...</td>\n","      <td>ive been walk same way as i did miss out crack...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Adele</td>\n","      <td>19</td>\n","      <td>2008</td>\n","      <td>Make You Feel My Love</td>\n","      <td>When the rain is blowing in your face\\nAnd the...</td>\n","      <td>when rain is blow your face and whole world is...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Adele</td>\n","      <td>19</td>\n","      <td>2008</td>\n","      <td>My Same</td>\n","      <td>Aye, aye, aye-aye\\nAye, aye, aye-aye\\nAye, aye...</td>\n","      <td>aye aye ayeay aye aye ayeay aye aye ayeay aye ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4907</th>\n","      <td>4908</td>\n","      <td>XXXTentacion</td>\n","      <td>The Fall</td>\n","      <td>2014</td>\n","      <td>The Fall</td>\n","      <td>Fool's gold is a common man's trash\\nI've seen...</td>\n","      <td>fool gold is a common man trash ive seen god i...</td>\n","    </tr>\n","    <tr>\n","      <th>4908</th>\n","      <td>4909</td>\n","      <td>XXXTentacion</td>\n","      <td>The Fall</td>\n","      <td>2014</td>\n","      <td>​ghost</td>\n","      <td>Shtuom ruo neewteb seid ecapS tuo dna ni revO\\...</td>\n","      <td>shtuom ruo neewteb seid ecap tuo dna ni revo m...</td>\n","    </tr>\n","    <tr>\n","      <th>4909</th>\n","      <td>4910</td>\n","      <td>XXXTentacion</td>\n","      <td>The Fall</td>\n","      <td>2014</td>\n","      <td>​white girl</td>\n","      <td>Haha\\nYou know, gang, gang, bitch\\nXXX, pussy ...</td>\n","      <td>haha you know gang gang bitch xxx pussi boy fa...</td>\n","    </tr>\n","    <tr>\n","      <th>4910</th>\n","      <td>4911</td>\n","      <td>XXXTentacion</td>\n","      <td>Willy Wonka Was a Child Murderer</td>\n","      <td>2016</td>\n","      <td>Willy Wonka Was a Child Murderer</td>\n","      <td>Yeah\\nIt's all in my, it's all in my head\\nIt'...</td>\n","      <td>yeah it all my it all my head it all my it all...</td>\n","    </tr>\n","    <tr>\n","      <th>4911</th>\n","      <td>4912</td>\n","      <td>XXXTentacion</td>\n","      <td>XXX</td>\n","      <td>2014</td>\n","      <td>DUTTY (Freestyle)</td>\n","      <td>Jah, Jah, Jah, ha-ha\\nSee me\\nHip-Hop's a jung...</td>\n","      <td>jah jah jah haha see me hiphop a jungl im deep...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4912 rows × 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-168bc780-770f-43c9-9301-077d68809a87')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-168bc780-770f-43c9-9301-077d68809a87 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-168bc780-770f-43c9-9301-077d68809a87');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c930d8e6-8ab6-4b7a-a354-7acf51efaa51\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c930d8e6-8ab6-4b7a-a354-7acf51efaa51')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c930d8e6-8ab6-4b7a-a354-7acf51efaa51 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_9c82689b-a11f-4c49-a55f-508b4c8ccb05\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('lyrics_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_9c82689b-a11f-4c49-a55f-508b4c8ccb05 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('lyrics_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"lyrics_df","summary":"{\n  \"name\": \"lyrics_df\",\n  \"rows\": 4912,\n  \"fields\": [\n    {\n      \"column\": \"no\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1418,\n        \"min\": 1,\n        \"max\": 4912,\n        \"num_unique_values\": 4912,\n        \"samples\": [\n          3149,\n          3185,\n          765\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"artist\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Taylor Swift\",\n          \"Justin Bieber\",\n          \"Post Malone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"album\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 671,\n        \"samples\": [\n          \"NOW That\\u2019s What I Call Music! 65 [US]\",\n          \"Dance_Again_The_Hits\",\n          \"Songs I Wrote With Amy - EP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 755,\n        \"min\": 0,\n        \"max\": 2025,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          1993,\n          2003,\n          1996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4746,\n        \"samples\": [\n          \"\\u200bpositions\",\n          \"Welcome To Heartbreak\",\n          \"Don\\u2019t (Rick Ross Remix)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lyrics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4740,\n        \"samples\": [\n          \"it could be worse i could be alone i could be locked in here on my own or like a stone that suddenly drops it never stops no   i could be lost or i could be saved calling out from beneath the waves beaten down by this ocean rain never again never again   screaming out from the crests of waves   it could be worse bitter or sweet it could be snapped from the jaws of defeat or like a light lit up on a beach wear your heart on your sleeve  4 you want to stop before you begin you want to sink when you know you could swim you want to stop just before you begin never give in never give in   screaming out from the crests of waves   nothing matters except life and the love you make nothing matters except life and the love you make nothing matters except life and the love you make except life and the love you make   screaming out from the crests of waves   screaming out from the crests of waves you're longing to be saved screaming out from the crests of waves you are longing to be saved screaming out from the crests of waves\",\n          \"unruly unruly maddas who keeps bringing more i've had too many this virginia done me off already i'm blem for real i might just say how i feel i'm blem for real i might just say how i feel   don't switch on me i got big plans we need to forward to the islands and get you gold no spray tans i need you to stop runnin' back to your ex he's a wasteman i wanna know how come we can never slash and stay friends i'm blem for real i might just say how i feel i'm blem for real i might just say how i feel   'cause i know what i like i know how i wanna live my life i don't need no advice you're not here and we both know why so move from me when you're extra move from me with the passa i'm buildin' up a house where they raised me you move with me i'll go crazy   don't switch on me i got big plans we need to forward to the islands and get you gold no spray tans i need you to stop runnin' back to your ex he's a wasteman i wanna know how come we can never slash and stay friends i'm blem for real i might just say how i feel i'm blem for real i might just say how i feel   i know we can't keep it together forever 'cause you're crazy sometimes and i only see you sometimes move from me when you're extra move from me with the passa i'm buildin' up a house where they raised me you move with me i'll go look just   don't switch on me i got big plans we need to forward to the islands and get you gold no spray tans i need you to stop runnin' back to your ex he's a wasteman i wanna know how come we can never slash and stay friends i'm blem for real i might just say how i feel i'm blem for real i might just say how i feel   lil wayne  lionel richie together forever good morning good afternoon good night i'm here to talk about more life one second\",\n          \"\\nTurn my lights on\\nHow the fuck you quiet with the mic on?\\nI don't get anxiety, you Sam Bowie-ass niggas\\nI just get my Mike on\\nY'all said I wouldn't go nowhere, I took the detour\\nWhen you see the someone in the crack right by the seashore?\\nWhen you see them brand new le Fleurs on the floor?\\nIf the cops ask my name, bitch, I'm Igor\\n\\nYeah, ayo\\nYeah, yeah\\nLet's go, let's go, I ain't playin' around\\nRed nose, red nose, all you niggas is clowns\\nNiggas turning it up, well, shit, I'm tearing it down\\nHard to believe in God when there ain't no mirrors around\\nWhat's up?\\n\\nUh-huh, ayo\\nUh-huh\\nYeah, yo\\nBitch\\nSee Tyler, The Creator LiveGet tickets as low as $63You might also like\\nRunning 'til the rims fall\\nHad them niggas and the cops looking jigsaw\\nI done fucked around and turned into the big dog\\nBetter get God, get caught? Bitch, I think not\\nYeah, new suit, new boots, same nigga, like what?\\nLukewarm-ass niggas always wanna talk\\nI'm hot, I'm heat to the core like Earth\\nDon't touch, don't go, niggas might get buck\\n\\nYo, yeah, ayo\\nYeah, yeah\\nLet's go, let's go, I ain't playin' around\\nRed nose, red nose, all you niggas is clowns\\nNiggas turning it up, well, shit, I'm tearing it down\\nHard to believe in God when there ain't no mirrors around\\nWhat's up?\\n\\nWhoop, uh-huh\\nYeah, bitch\\nYeah\\n\\nI see the light\\nNow didn't I tell you motherfuckers\\nUm, I see the light\\nTo pay attention and to keep your motherfucking eyes glued to the man that's in front of you?\\nI see the light\\nThat's what the fuck I expect all y'all motherfuckers to do\\nI see the light\\n\\nDracula, Dracula, Dracula\\nSuck me first, I might get back at ya (I see the light)\\nIs that shit clear? Check the aperture\\nHahahahaha, I can't laugh at ya (I see the light)\\nThis the shit that make you nervous\\n'Bout to go buck wild, nigga, Steve Irwin (I see the light)\\nSick of that 'Claren talk, I'm on my third one\\nNiggas talkin' reckless, I never heard 'em (I see the light)\\n\\nI see the light\\nI see the light\\nI see the light\\n(Buck, don't touch, though, my niggas might get buck, don't touch, though)\\nI see the light\\n(Buck, don't touch, though, my niggas might get buck, don't touch, though)\\nI see the light\\nThat car crash couldn't take me (Woo, ha)\\nGreen haired angels all around me (Uh)\\nNo answer why, no tears to cry, bitch, I'm alive (I see the)\\nThat wasn't my endpoint like v-neck\\nI ain't have nobody to cheat on, I cheat death\\nNew album, no repeat, I reset\\nEverything I deliver special like G. Dep\\nTwo of 'em, I total, Kima, Pam\\nMe and death, the universe played middle man\\nQuick nap, kick back like horse, eyes shut\\nLoud sound, no scratch\\nMotherfuckers really thought I died\\nHoping they could take a spot\\nNigga not knowing that I'm one of one\\nAnd they some Helen Keller-ass niggas\\nAnd I got my eyes open, now I see the\\n\\nLight\\nI see the light\\nYou never wanna meet a motherfucker like me\\nI see the light (I said)\\nI see the light\\nI see the light\\nI don't know what's harder, letting go or just being okay with it62Embed\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processed_Lyrics\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4732,\n        \"samples\": [\n          \"hahaha word up you know you like a donald goin nigga that off top i got bill and shit to pay and anoth mouth on way im ghetto that right that what im talkin bout tell tell young boy out there man takin it back on some soul type shit hiphop it rare form spend a dollar on some wine hand my cloth out on line im ghetto now yall nigga wan na wear polo and hilfig dreamin all of thi rich dreamin all of thi bitch see me cruisin down lane doin two mile per hour babi im ghetto yall wan na go place like vega and beverli hill like everi three second we just wan na talk to yall tell yall a lil somethin im not asham of my fam i cant help it who i am im ghetto i tell stori for young black youth out there yo it start out east new york where i grew up knew how to walk favorit carri weapon wa a fork older folk smoke out stragler almost had to bust a marvin haglar starvin for mine bust three at ya dad fuck up consecut him and some rel is gangster sellin pussi mom drop him on spot smack me knot onli he gave you wa your name and these block that ya pop still that me still keep it real if it wasnt for him and hi steel there probabl wouldnt be a meal we hi caddi silicon out zone out three day away home he stone out coke on tabl pressin redial son im sendin you home nigga you foul but gettin big half nigga i run wit gettin jig other fifti cent balancin a bid oh shit got thi bitch seed cop plead what to do knock your self off god succeed and most scariest part better have heart caus lion and wolv will rip you apart make a brick and get bar shoot joint get involv put tint on your window whip car yeah all giant revolv black carryon readi to die for famili hard alcohol vultur toaster sheep skin on leather v bomber bum no coat on we livin financi fuck up got ta have credit smoke a bag of wet and set it block we dead societi wrong we too black and too strong to prolong got my woman a gun her thong it kinda wild how we livin devil fall ha arisen we travel here to prison a presid shelter for my elder that real caus we need them and they need us that wealth for ya leavin on thi moment medit four mornin nigga you straight you might also like i got hustlin on my mind otherwis im feelin fine im ghetto i like black tint on my car at park a superstar im ghetto yeah see me cruisin down lane two mile per hour babi im ghetto im not asham of my fam i cant help it who i am im ghetto born gutter a black pop and a puerto rican mother i kept a stolen whip and razor sharp box cutter known albani ave to th and sutter my leather wa pleather while yall wa butter robbin and hustlin anyth for a profit thirteen year old month spofford crime i couldnt stop it didnt pass tenth grade school wa cool but it didnt keep our rent paid so everi wednesday me and my dog hop train for a ki bronx knowin all along shit wa wrong aint first and damn sure aint last it keep goin on so keep movin on same old stori my pop left wave and poni and mock neck tre and forti and hot sex around way bitch around way rich yo my block rep i stay brooklyn zone underground is how i network million dollar meet timb and sweatshirt so pardon my appear everyth all good nigga im just like you except im hood i got bill and shit to pay and anoth mouth on way im ghetto spend a dollar on some wine hang my cloth out on line im ghetto see me cruisin down lane doin two mile per hour babi im ghetto im not asham of my fam i cant help it who i am im ghetto doe anyon mind truth yall doe anyon mind truth yall doe anyon mind truth yall doe anyon mind truth yallemb\",\n          \"unruli unruli madda who keep bring more ive had too mani thi virginia done me off alreadi im blem for real i might just say how i feel im blem for real i might just say how i feel dont switch on me i got big plan we need to forward to island and get you gold no spray tan i need you to stop runnin back to your ex he a wasteman i wan na know how come we can never slash and stay friend im blem for real i might just say how i feel im blem for real i might just say how i feel caus i know what i like i know how i wan na live my life i dont need no advic your not here and we both know whi so move me when your extra move me passa im buildin up a hous where they rais me you move me ill go crazi dont switch on me i got big plan we need to forward to island and get you gold no spray tan i need you to stop runnin back to your ex he a wasteman i wan na know how come we can never slash and stay friend im blem for real i might just say how i feel im blem for real i might just say how i feel i know we cant keep it togeth forev caus your crazi sometim and i onli see you sometim move me when your extra move me passa im buildin up a hous where they rais me you move me ill go look just dont switch on me i got big plan we need to forward to island and get you gold no spray tan i need you to stop runnin back to your ex he a wasteman i wan na know how come we can never slash and stay friend im blem for real i might just say how i feel im blem for real i might just say how i feel lil wayn lionel richi togeth forev good morn good afternoon good night im here to talk about more life one second\",\n          \"wherev your go i wan na go wherev your head can you let me know dont mind catch up im on my way just cant take thought of you mile away pre and i know your go somewher to make a better life i hope that youll find it on first tri and even though it kill me that you have to go i know itll be sadder if you never hit road so farewel somebodi gon na miss you miss you miss you miss you farewel somebodi gon na wish that you were here that somebodi me i will write to tell you what goin on but you wont miss noth but same old if you dont mind catch up ill spend day tell you stori about a land far away but i know pre and i know your go somewher to make a better life oh i hope that youll find it on first tri and even though it kill me that you have to go i know itll be sadder if you never hit road so farewel somebodi gon na miss you miss you miss you miss you farewel somebodi gon na wish that you were here farewel somebodi gon na miss you miss you miss you miss you farewel somebodi gon na wish that you were here that somebodi me and im gon tri to hold it all tri to hold back my tear so it dont make you stay here yeah imma tri to be a big girl now caus i dont wan na be reason you dont leav farewel somebodi gon na miss you miss you miss you miss you farewel somebodi gon na wish that you were here farewel somebodi gon na miss you miss you miss you miss you farewel somebodi gon na wish that you were here that somebodi me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["data = lyrics_df"],"metadata":{"id":"YZxmbtz1H8aK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import networkx as nx\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","kg = nx.Graph()\n","\n","for _, row in data.iterrows():\n","    song_id = row['no']\n","\n","    kg.add_node(row['title'], type='song', song_id=song_id)\n","    kg.add_node(row['artist'], type='artist')\n","    kg.add_edge(row['title'], row['artist'], relation='sung_by')\n","\n","    kg.add_node(row['lyrics'], type='lyrics')\n","    kg.add_edge(row['title'], row['lyrics'], relation='has_lyric')\n","\n","    if pd.notnull(row.get('album', None)):\n","        album = row['album']\n","        kg.add_node(album, type='album')\n","        kg.add_edge(row['title'], album, relation='has_album')\n","        kg.add_edge(row['artist'], album, relation='created_album')\n","\n","    if pd.notnull(row.get('year', None)):\n","        year = row['year']\n","        kg.add_node(year, type='year')\n","        kg.add_edge(row['title'], year, relation='released_in')\n","        kg.add_edge(row['artist'], year, relation='active_in')\n","\n","vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = vectorizer.fit_transform(data['Processed_Lyrics'])\n","\n","cosine_sim = cosine_similarity(tfidf_matrix)\n","\n","threshold = 0.1\n","\n","for i in range(len(data)):\n","    for j in range(i + 1, len(data)):\n","        score = cosine_sim[i, j]\n","        if score > threshold:\n","            song1 = data.iloc[i]['title']\n","            song2 = data.iloc[j]['title']\n","\n","            if not kg.has_edge(song1, song2):\n","                kg.add_edge(song1, song2, relation='related_to', similarity_score=score)\n","\n","nx.write_graphml(kg, \"K_Graph.graphml\")\n","\n","print(\"Graph berhasil disimpan.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEiX6v_6H60z","executionInfo":{"status":"ok","timestamp":1734559593667,"user_tz":-420,"elapsed":247462,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"30919599-cdb4-41e2-c8a9-4f7bf2ae7bad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Graph berhasil disimpan.\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def precision_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Calculate precision at k.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    k: The number of top retrieved documents to evaluate.\n","    \"\"\"\n","    if k == 0:\n","        return 0.0\n","    retrieved_k = retrieved[:k]\n","\n","    if isinstance(relevant, set):\n","        relevant = list(relevant)\n","\n","    if isinstance(retrieved_k[0], dict):\n","        retrieved_k = [tuple(d.items()) for d in retrieved_k]\n","    if isinstance(relevant[0], dict):\n","        relevant = [tuple(d.items()) for d in relevant]\n","\n","    relevant_retrieved = set(retrieved_k).intersection(relevant)\n","    return len(relevant_retrieved) / k\n","\n","\n","def recall_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Calculate recall at k.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    k: The number of top retrieved documents to evaluate.\n","    \"\"\"\n","    if len(relevant) == 0:\n","        return 0.0\n","    retrieved_k = retrieved[:k]\n","\n","    relevant_retrieved = set(retrieved_k).intersection(relevant)\n","    return len(relevant_retrieved) / len(relevant)\n","\n","\n","def f1_at_k(precision, recall):\n","    \"\"\"\n","    Calculate the F1 score at k.\n","    precision: Precision value.\n","    recall: Recall value.\n","    \"\"\"\n","    if precision + recall == 0:\n","        return 0.0\n","    return 2 * (precision * recall) / (precision + recall)\n","\n","\n","def average_precision_func(relevant, retrieved):\n","    \"\"\"\n","    Calculate average precision for a query.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    \"\"\"\n","    if len(relevant) == 0:\n","        return 0.0\n","\n","    ap = 0.0\n","    num_relevant = 0\n","    for i, doc_id in enumerate(retrieved, start=1):\n","        if doc_id in relevant:\n","            num_relevant += 1\n","            ap += num_relevant / i\n","\n","    return ap / len(relevant)\n","\n","\n","def evaluate_query(relevant_ids, retrieved_docs, k_values=[3, 6, 10]):\n","    \"\"\"\n","    Evaluate a single query's results at given k-values and return metrics in a dictionary.\n","    relevant_ids: List of relevant document IDs.\n","    retrieved_docs: List of retrieved document IDs.\n","    k_values: List of k-values for evaluating precision and recall at different cut-off points.\n","    \"\"\"\n","    evaluation = {}\n","    for k in k_values:\n","        p = precision_at_k(relevant_ids, retrieved_docs, k)\n","        r = recall_at_k(relevant_ids, retrieved_docs, k)\n","        f1 = f1_at_k(p, r)\n","\n","        evaluation[f'Precision@{k}'] = p\n","        evaluation[f'Recall@{k}'] = r\n","        evaluation[f'F1@{k}'] = f1\n","\n","    ap = average_precision_func(relevant_ids, retrieved_docs)\n","    evaluation['Average Precision'] = ap\n","\n","    return evaluation\n","\n","\n","def retrieve_with_tfidf(data_df, query, ngram_range=(1, 1), top_k=10):\n","    \"\"\"\n","    Retrieve the top-k most relevant documents based on TF-IDF cosine similarity.\n","    data_df: DataFrame containing the documents.\n","    query: The query string.\n","    ngram_range: The range of n-grams for TF-IDF.\n","    top_k: The number of top documents to return.\n","    \"\"\"\n","    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(data_df['Processed_Lyrics'])\n","\n","    query_vector = tfidf_vectorizer.transform([query])\n","    cosine_sim = cosine_similarity(query_vector, tfidf_matrix).flatten()\n","\n","    top_indices = cosine_sim.argsort()[-top_k:][::-1]\n","\n","    retrieved_docs = []\n","    for idx in top_indices:\n","        doc = data_df.iloc[idx]\n","        retrieved_docs.append({\n","            'id': doc['no'],\n","            'artist': doc.get('artist', None),\n","            'album': doc.get('album', None),\n","            'tfidf_score': cosine_sim[idx]\n","        })\n","    return retrieved_docs\n"],"metadata":{"id":"p_5Pk6R3A-Ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_artist_graph_score(artist, kg, query):\n","    \"\"\"\n","    Menghitung skor berdasarkan apakah artis ada dalam graph dan relevansi dengan query.\n","    Jika artis ada dalam graph dan relevan dengan query, skor ditambah.\n","    \"\"\"\n","    if kg.has_node(artist):\n","        if artist.lower() in query.lower():\n","            return 1\n","    return 0\n","\n","def get_album_graph_score(album, kg, query):\n","    \"\"\"\n","    Menghitung skor berdasarkan apakah album ada dalam graph dan relevansi dengan query.\n","    Jika album ada dalam graph dan relevan dengan query, skor ditambah.\n","    \"\"\"\n","    if kg.has_node(album):\n","        if album.lower() in query.lower():\n","            return 1\n","    return 0\n","\n","\n","def parse_query_for_graph(query, data):\n","    \"\"\"\n","    Analisis query untuk menemukan artis, album, dan tahun.\n","    \"\"\"\n","    artist = None\n","    album = None\n","    year = None\n","\n","    for known_artist in data['artist']:\n","        if known_artist.lower() in query.lower():\n","            artist = known_artist\n","            break\n","\n","    for known_album in data['album']:\n","        if pd.notnull(known_album) and known_album.lower() in query.lower():\n","            album = known_album\n","            break\n","\n","    for known_year in data['year']:\n","        if str(known_year) in query:\n","            year = known_year\n","            break\n","\n","    return artist, album, year"],"metadata":{"id":"K7DvZMbMIbPM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","k_values = [3, 6, 10]\n","top_k = 10\n","all_ngram_evaluations = []\n","\n","for n in range(1, 6):\n","    evaluation_results = []\n","\n","    for idx, row in ground_truth_df.iterrows():\n","        query = row['Processed_Query']\n","        relevant_ids = set(row['Relevant_Song_IDs'])\n","\n","        artist, album, year = parse_query_for_graph(query, lyrics_df)\n","\n","        tfidf_retrieved_docs = retrieve_with_tfidf(\n","            lyrics_df, query, ngram_range=(n, n), top_k=top_k\n","        )\n","\n","        for doc in tfidf_retrieved_docs:\n","            doc_id = doc['id']\n","\n","            if doc_id in lyrics_df['no'].values:\n","                lyric_row = lyrics_df.loc[lyrics_df['no'] == doc_id]\n","                artist_name = lyric_row['artist'].values[0] if not lyric_row['artist'].isnull().all() else None\n","                album_name = lyric_row['album'].values[0] if not lyric_row['album'].isnull().all() else None\n","\n","                if artist_name:\n","                    doc['artist_score'] = get_artist_graph_score(artist_name, kg, query)\n","                else:\n","                    print(f\"Warning: artist_name is invalid for doc_id {doc_id}\")\n","                    doc['artist_score'] = 0\n","\n","                if album_name:\n","                    doc['album_score'] = get_album_graph_score(album_name, kg, query)\n","                else:\n","                    print(f\"Warning: album_name is invalid for doc_id {doc_id}\")\n","                    doc['album_score'] = 0\n","\n","                doc['combined_score'] = (\n","                    doc.get('tfidf_score', 0)\n","                    + doc.get('artist_score', 0)\n","                    + doc.get('album_score', 0)\n","                )\n","            else:\n","                print(f\"Warning: doc_id {doc_id} not found in lyrics_df\")\n","\n","        combined_retrieved_docs = sorted(\n","            tfidf_retrieved_docs, key=lambda x: x['combined_score'], reverse=True\n","        )[:top_k]\n","        combined_retrieved_ids = [doc['id'] for doc in combined_retrieved_docs]\n","\n","        evaluation = evaluate_query(relevant_ids, combined_retrieved_ids, k_values)\n","        evaluation['Query'] = row['Processed_Query']\n","        evaluation['N-gram'] = n\n","        evaluation_results.append(evaluation)\n","\n","    eval_df = pd.DataFrame(evaluation_results)\n","    print(f\"\\nEvaluation Results for {n}-gram:\")\n","    print(eval_df.head())\n","\n","    macro_results = {\n","        \"Metric\": [\n","            \"Precision@3\", \"Recall@3\", \"F1@3\",\n","            \"Precision@6\", \"Recall@6\", \"F1@6\",\n","            \"Precision@10\", \"Recall@10\", \"F1@10\",\n","            \"Average Precision\"\n","        ],\n","        \"Macro Average\": [\n","            eval_df['Precision@3'].mean(),\n","            eval_df['Recall@3'].mean(),\n","            eval_df['F1@3'].mean(),\n","            eval_df['Precision@6'].mean(),\n","            eval_df['Recall@6'].mean(),\n","            eval_df['F1@6'].mean(),\n","            eval_df['Precision@10'].mean(),\n","            eval_df['Recall@10'].mean(),\n","            eval_df['F1@10'].mean(),\n","            eval_df['Average Precision'].mean()\n","        ]\n","    }\n","    macro_df = pd.DataFrame(macro_results)\n","    macro_df['N-gram'] = n\n","    print(f\"\\nMacro Averages for {n}-gram:\")\n","    print(macro_df)\n","\n","    eval_df.to_excel(f'evaluation_combined_tfidf_graph_{n}gram.xlsx', index=False)\n","    macro_df.to_excel(f'macro_averages_combined_graph_{n}gram.xlsx', index=False)\n","\n","    all_ngram_evaluations.append(macro_df)\n","\n","combined_macro = pd.concat(all_ngram_evaluations, ignore_index=True)\n","combined_macro.to_excel('combined_macro_averages_all_ngrams_with_graph.xlsx', index=False)\n","print(\"\\nAll macro averages for all n-grams have been saved to 'combined_macro_averages_all_ngrams_with_graph.xlsx'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GV4dMoC27QJm","executionInfo":{"status":"ok","timestamp":1734560628100,"user_tz":-420,"elapsed":1023876,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"9ae4715a-5cc9-4911-c9a4-aef2d1cdcdc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results for 1-gram:\n","   Precision@3  Recall@3      F1@3  Precision@6  Recall@6      F1@6  \\\n","0     0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n","1     0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n","2     0.333333  0.055556  0.095238     0.333333  0.111111  0.166667   \n","3     0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n","4     0.000000  0.000000  0.000000     0.166667  0.333333  0.222222   \n","\n","   Precision@10  Recall@10     F1@10  Average Precision         Query  N-gram  \n","0           0.0   0.000000  0.000000           0.000000      love you       1  \n","1           0.0   0.000000  0.000000           0.000000  home tonight       1  \n","2           0.2   0.111111  0.142857           0.074074    someth new       1  \n","3           0.1   0.250000  0.142857           0.031250    i held you       1  \n","4           0.1   0.333333  0.153846           0.066667  star tonight       1  \n","\n","Macro Averages for 1-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.120000       1\n","1           Recall@3       0.056667       1\n","2               F1@3       0.074545       1\n","3        Precision@6       0.133333       1\n","4           Recall@6       0.151667       1\n","5               F1@6       0.133365       1\n","6       Precision@10       0.088000       1\n","7          Recall@10       0.166111       1\n","8              F1@10       0.108785       1\n","9  Average Precision       0.085639       1\n","\n","Evaluation Results for 2-gram:\n","   Precision@3  Recall@3      F1@3  Precision@6  Recall@6      F1@6  \\\n","0     0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n","1     1.000000  0.250000  0.400000     1.000000  0.500000  0.666667   \n","2     1.000000  0.166667  0.285714     1.000000  0.333333  0.500000   \n","3     0.666667  0.500000  0.571429     0.666667  1.000000  0.800000   \n","4     1.000000  1.000000  1.000000     0.500000  1.000000  0.666667   \n","\n","   Precision@10  Recall@10     F1@10  Average Precision         Query  N-gram  \n","0           0.0   0.000000  0.000000           0.000000      love you       2  \n","1           1.0   0.833333  0.909091           0.833333  home tonight       2  \n","2           1.0   0.555556  0.714286           0.555556    someth new       2  \n","3           0.4   1.000000  0.571429           0.854167    i held you       2  \n","4           0.3   1.000000  0.461538           1.000000  star tonight       2  \n","\n","Macro Averages for 2-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.533333       2\n","1           Recall@3       0.340190       2\n","2               F1@3       0.381377       2\n","3        Precision@6       0.453333       2\n","4           Recall@6       0.495937       2\n","5               F1@6       0.436010       2\n","6       Precision@10       0.364000       2\n","7          Recall@10       0.609968       2\n","8              F1@10       0.420582       2\n","9  Average Precision       0.530985       2\n","\n","Evaluation Results for 3-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision         Query  N-gram  \n","0        0.0    0.0                0.0      love you       3  \n","1        0.0    0.0                0.0  home tonight       3  \n","2        0.0    0.0                0.0    someth new       3  \n","3        0.0    0.0                0.0    i held you       3  \n","4        0.0    0.0                0.0  star tonight       3  \n","\n","Macro Averages for 3-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.320000       3\n","1           Recall@3       0.229571       3\n","2               F1@3       0.251792       3\n","3        Precision@6       0.260000       3\n","4           Recall@6       0.324143       3\n","5               F1@6       0.273487       3\n","6       Precision@10       0.204000       3\n","7          Recall@10       0.400000       3\n","8              F1@10       0.258898       3\n","9  Average Precision       0.347665       3\n","\n","Evaluation Results for 4-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision         Query  N-gram  \n","0        0.0    0.0                0.0      love you       4  \n","1        0.0    0.0                0.0  home tonight       4  \n","2        0.0    0.0                0.0    someth new       4  \n","3        0.0    0.0                0.0    i held you       4  \n","4        0.0    0.0                0.0  star tonight       4  \n","\n","Macro Averages for 4-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.040000       4\n","1           Recall@3       0.024000       4\n","2               F1@3       0.030000       4\n","3        Precision@6       0.046667       4\n","4           Recall@6       0.066667       4\n","5               F1@6       0.054141       4\n","6       Precision@10       0.032000       4\n","7          Recall@10       0.080000       4\n","8              F1@10       0.045128       4\n","9  Average Precision       0.053492       4\n","\n","Evaluation Results for 5-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision         Query  N-gram  \n","0        0.0    0.0                0.0      love you       5  \n","1        0.0    0.0                0.0  home tonight       5  \n","2        0.0    0.0                0.0    someth new       5  \n","3        0.0    0.0                0.0    i held you       5  \n","4        0.0    0.0                0.0  star tonight       5  \n","\n","Macro Averages for 5-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.040000       5\n","1           Recall@3       0.040000       5\n","2               F1@3       0.040000       5\n","3        Precision@6       0.020000       5\n","4           Recall@6       0.040000       5\n","5               F1@6       0.026667       5\n","6       Precision@10       0.012000       5\n","7          Recall@10       0.040000       5\n","8              F1@10       0.018462       5\n","9  Average Precision       0.040000       5\n","\n","All macro averages for all n-grams have been saved to 'combined_macro_averages_all_ngrams_with_graph.xlsx'.\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def rrf_fusion(ranked_lists, rrf_k=60):\n","    \"\"\"\n","    Perform Reciprocal Rank Fusion (RRF) on multiple ranked lists.\n","\n","    Parameters:\n","    - ranked_lists: List of lists, where each sublist contains document IDs ordered by relevance.\n","    - rrf_k: Constant to control the influence of rank.\n","\n","    Returns:\n","    - fused_docs: List of document IDs ordered by their RRF scores.\n","    \"\"\"\n","    rrf_scores = defaultdict(float)\n","    for ranked_list in ranked_lists:\n","        for rank, doc in enumerate(ranked_list):\n","            if isinstance(doc, dict):\n","                doc_id = doc.get('id')\n","            else:\n","                doc_id = doc\n","            rrf_scores[doc_id] += 1.0 / (rrf_k + rank + 1)\n","    fused_ranking = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n","    fused_docs = [doc_id for doc_id, score in fused_ranking]\n","    return fused_docs\n","\n","\n","rrf_evaluations = []\n","\n","for idx, row in ground_truth_df.iterrows():\n","    query = row['Processed_Query']\n","    relevant_ids = set(row['Relevant_Song_IDs'])\n","\n","    ranked_lists = []\n","    for n in range(1, 6):\n","        retrieved_docs = retrieve_with_tfidf(lyrics_df, query, ngram_range=(n,n), top_k=top_k)\n","        ranked_lists.append(retrieved_docs)\n","\n","    fused_docs = rrf_fusion(ranked_lists, rrf_k=60)\n","\n","    rrf_eval = evaluate_query(relevant_ids, fused_docs, k_values)\n","    rrf_eval['Query'] = row['query']\n","    rrf_evaluations.append(rrf_eval)\n","\n","rrf_df = pd.DataFrame(rrf_evaluations)\n","print(\"\\nRRF Fused Evaluation Results:\")\n","print(rrf_df.head())\n","\n","rrf_macro_results = {\n","    \"Metric\": [\n","        \"Precision\",\n","        \"Recall\",\n","        \"F1\",\n","        \"Precision@3\",\n","        \"Precision@6\",\n","        \"Precision@10\",\n","        \"Average Precision\"\n","    ],\n","    \"Macro Average\": [\n","        rrf_df['Precision@10'].mean(),\n","        rrf_df['Recall@10'].mean(),\n","        rrf_df['F1@10'].mean(),\n","        rrf_df['Precision@3'].mean(),\n","        rrf_df['Precision@6'].mean(),\n","        rrf_df['Precision@10'].mean(),\n","        rrf_df['Average Precision'].mean()\n","    ]\n","}\n","rrf_macro_df = pd.DataFrame(rrf_macro_results)\n","rrf_macro_df['N-gram'] = 'RRF Fusion'\n","print(\"\\nMacro Averages for RRF Fusion:\")\n","print(rrf_macro_df)\n","\n","rrf_df.to_excel('evaluation_tfidf_rrf_fusion.xlsx', index=False)\n","rrf_macro_df.to_excel('macro_averages_rrf_fusion.xlsx', index=False)\n","print(\"\\nRRF Fused evaluation results have been saved to 'evaluation_tfidf_rrf_fusion.xlsx'.\")\n","print(\"Macro averages for RRF Fusion have been saved to 'macro_averages_rrf_fusion.xlsx'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muMs7lb_zljm","executionInfo":{"status":"ok","timestamp":1734561932826,"user_tz":-420,"elapsed":1063713,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"6df25c3b-b52d-461b-a342-c89b9c1936dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","RRF Fused Evaluation Results:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision          Query  \n","0        0.0    0.0           0.000000    love you in  \n","1        0.0    0.0           0.201063   home tonight  \n","2        0.0    0.0           0.149886  something new  \n","3        0.0    0.0           0.148310     i held you  \n","4        0.0    0.0           0.148252  stars tonight  \n","\n","Macro Averages for RRF Fusion:\n","              Metric  Macro Average      N-gram\n","0          Precision       0.088000  RRF Fusion\n","1             Recall       0.187857  RRF Fusion\n","2                 F1       0.115698  RRF Fusion\n","3        Precision@3       0.106667  RRF Fusion\n","4        Precision@6       0.073333  RRF Fusion\n","5       Precision@10       0.088000  RRF Fusion\n","6  Average Precision       0.226478  RRF Fusion\n","\n","RRF Fused evaluation results have been saved to 'evaluation_tfidf_rrf_fusion.xlsx'.\n","Macro averages for RRF Fusion have been saved to 'macro_averages_rrf_fusion.xlsx'.\n"]}]},{"cell_type":"markdown","source":["# Complex"],"metadata":{"id":"c5G4VSueR0FH"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import warnings\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from collections import defaultdict\n","import math\n","from sklearn.metrics.pairwise import cosine_similarity\n","warnings.filterwarnings(\"ignore\")\n","\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","\n","lyrics_df = pd.read_excel('/content/databersih (2).xlsx')\n","ground_truth_df = pd.read_excel('/content/GT UAS NLP_complex.xlsx')\n","\n","stemmer = PorterStemmer()\n","remove_words = [\"song\", \"with\", \"lyrics\", \"from\", \"the\", \"album\", \"released\", \"in\", \"before\", \"after\", \"since\", \"s\"]"],"metadata":{"id":"vLPyay8XDM_A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734562014786,"user_tz":-420,"elapsed":3651,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"6bc14c83-4720-4631-a21f-742ac42d7b88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["def clean_lyrics(text):\n","    if isinstance(text, str):\n","        text = re.sub(r'\\[.*?\\]', '', text)\n","        text = re.sub(r'\\(.*?\\)', '', text)\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        text = re.sub(r'\\d+', '', text)\n","        text = text.lower()\n","        tokens = word_tokenize(text)\n","        tokens = [stemmer.stem(word) for word in tokens if word not in remove_words]\n","        text = ' '.join(tokens)\n","    return text\n","\n","lyrics_df['Processed_Lyrics'] = lyrics_df['lyrics'].apply(clean_lyrics)\n","\n","print(\"\\nCleaned Lyrics Sample:\")\n","print(lyrics_df[['title', 'Processed_Lyrics']].head())\n","\n","def parse_song_ids(song_id_entry):\n","    if pd.isnull(song_id_entry):\n","        return []\n","    if isinstance(song_id_entry, int):\n","        return [song_id_entry]\n","    if isinstance(song_id_entry, float) and np.isnan(song_id_entry):\n","        return []\n","    song_id_str = str(song_id_entry)\n","    return [int(id_.strip()) for id_ in song_id_str.split(',') if id_.strip().isdigit()]\n","\n","ground_truth_df['Relevant_Song_IDs'] = ground_truth_df['song_id'].apply(parse_song_ids)\n","ground_truth_df = ground_truth_df[ground_truth_df['Relevant_Song_IDs'].map(len) > 0].reset_index(drop=True)\n","print(\"\\nParsed Ground Truth:\")\n","print(ground_truth_df[['query', 'Relevant_Song_IDs', 'total']].head())\n","\n","def preprocess_query(query):\n","    if isinstance(query, str):\n","        query = re.sub(r'[^\\w\\s]', '', query)\n","        query = re.sub(r'\\d+', '', query)\n","        query = query.lower()\n","        tokens = word_tokenize(query)\n","        tokens = [stemmer.stem(word) for word in tokens if word not in remove_words]\n","        return ' '.join(tokens)\n","    return query\n","\n","ground_truth_df['Processed_Query'] = ground_truth_df['query'].apply(preprocess_query)"],"metadata":{"id":"lypl9wWLDS6S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734562063686,"user_tz":-420,"elapsed":46870,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"80f987b0-841b-4881-b49f-b079a700031e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cleaned Lyrics Sample:\n","                   title                                   Processed_Lyrics\n","0      Chasing Pavements  ive made up my mind dont need to think it over...\n","1          Cold Shoulder  you say it all my head and thing i think just ...\n","2         Hometown Glory  ive been walk same way as i did miss out crack...\n","3  Make You Feel My Love  when rain is blow your face and whole world is...\n","4                My Same  aye aye ayeay aye aye ayeay aye aye ayeay aye ...\n","\n","Parsed Ground Truth:\n","                                               query Relevant_Song_IDs  total\n","0               Adele's song with lyrics love you in            [1433]      1\n","1    Billie Eilish's song with lyrics stick together             [227]      1\n","2              Bruno Mars' song with lyrics kiss you            [2129]      1\n","3    Jennifer Lopez's song with lyrics you feel left             [788]      1\n","4  Song released in 2024 with lyrics hopeless rom...            [3019]      1\n"]}]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def precision_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Calculate precision at k.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    k: The number of top retrieved documents to evaluate.\n","    \"\"\"\n","    if k == 0:\n","        return 0.0\n","    retrieved_k = retrieved[:k]\n","\n","    if isinstance(relevant, set):\n","        relevant = list(relevant)\n","\n","    if isinstance(retrieved_k[0], dict):\n","        retrieved_k = [tuple(d.items()) for d in retrieved_k]\n","    if isinstance(relevant[0], dict):\n","        relevant = [tuple(d.items()) for d in relevant]\n","\n","    relevant_retrieved = set(retrieved_k).intersection(relevant)\n","    return len(relevant_retrieved) / k\n","\n","\n","def recall_at_k(relevant, retrieved, k):\n","    \"\"\"\n","    Calculate recall at k.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    k: The number of top retrieved documents to evaluate.\n","    \"\"\"\n","    if len(relevant) == 0:\n","        return 0.0\n","    retrieved_k = retrieved[:k]\n","\n","    relevant_retrieved = set(retrieved_k).intersection(relevant)\n","    return len(relevant_retrieved) / len(relevant)\n","\n","\n","def f1_at_k(precision, recall):\n","    \"\"\"\n","    Calculate the F1 score at k.\n","    precision: Precision value.\n","    recall: Recall value.\n","    \"\"\"\n","    if precision + recall == 0:\n","        return 0.0\n","    return 2 * (precision * recall) / (precision + recall)\n","\n","\n","def average_precision_func(relevant, retrieved):\n","    \"\"\"\n","    Calculate average precision for a query.\n","    relevant: List of relevant document IDs.\n","    retrieved: List of retrieved document IDs.\n","    \"\"\"\n","    if len(relevant) == 0:\n","        return 0.0\n","\n","    ap = 0.0\n","    num_relevant = 0\n","    for i, doc_id in enumerate(retrieved, start=1):\n","        if doc_id in relevant:\n","            num_relevant += 1\n","            ap += num_relevant / i\n","\n","    return ap / len(relevant)\n","\n","\n","def evaluate_query(relevant_ids, retrieved_docs, k_values=[3, 6, 10]):\n","    \"\"\"\n","    Evaluate a single query's results at given k-values and return metrics in a dictionary.\n","    relevant_ids: List of relevant document IDs.\n","    retrieved_docs: List of retrieved document IDs.\n","    k_values: List of k-values for evaluating precision and recall at different cut-off points.\n","    \"\"\"\n","    evaluation = {}\n","    for k in k_values:\n","        p = precision_at_k(relevant_ids, retrieved_docs, k)\n","        r = recall_at_k(relevant_ids, retrieved_docs, k)\n","        f1 = f1_at_k(p, r)\n","\n","        evaluation[f'Precision@{k}'] = p\n","        evaluation[f'Recall@{k}'] = r\n","        evaluation[f'F1@{k}'] = f1\n","\n","    ap = average_precision_func(relevant_ids, retrieved_docs)\n","    evaluation['Average Precision'] = ap\n","\n","    return evaluation\n","\n","\n","def retrieve_with_tfidf(data_df, query, ngram_range=(1, 1), top_k=10):\n","    \"\"\"\n","    Retrieve the top-k most relevant documents based on TF-IDF cosine similarity.\n","    data_df: DataFrame containing the documents.\n","    query: The query string.\n","    ngram_range: The range of n-grams for TF-IDF.\n","    top_k: The number of top documents to return.\n","    \"\"\"\n","    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(data_df['Processed_Lyrics'])\n","\n","    query_vector = tfidf_vectorizer.transform([query])\n","    cosine_sim = cosine_similarity(query_vector, tfidf_matrix).flatten()\n","\n","    top_indices = cosine_sim.argsort()[-top_k:][::-1]\n","\n","    retrieved_docs = []\n","    for idx in top_indices:\n","        doc = data_df.iloc[idx]\n","        retrieved_docs.append({\n","            'id': doc['no'],\n","            'artist': doc.get('artist', None),\n","            'album': doc.get('album', None),\n","            'tfidf_score': cosine_sim[idx]\n","        })\n","    return retrieved_docs\n"],"metadata":{"id":"_bzyJx5TDWDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_artist_graph_score(artist, kg, query):\n","    \"\"\"\n","    Menghitung skor berdasarkan apakah artis ada dalam graph dan relevansi dengan query.\n","    Jika artis ada dalam graph dan relevan dengan query, skor ditambah.\n","    \"\"\"\n","    if kg.has_node(artist):\n","        if artist.lower() in query.lower():\n","            return 1\n","    return 0\n","\n","def get_album_graph_score(album, kg, query):\n","    \"\"\"\n","    Menghitung skor berdasarkan apakah album ada dalam graph dan relevansi dengan query.\n","    Jika album ada dalam graph dan relevan dengan query, skor ditambah.\n","    \"\"\"\n","    if kg.has_node(album):\n","        if album.lower() in query.lower():\n","            return 1\n","    return 0\n","\n","\n","def parse_query_for_graph(query, data):\n","    \"\"\"\n","    Analisis query untuk menemukan artis, album, dan tahun.\n","    \"\"\"\n","    artist = None\n","    album = None\n","    year = None\n","\n","    for known_artist in data['artist']:\n","        if known_artist.lower() in query.lower():\n","            artist = known_artist\n","            break\n","\n","    for known_album in data['album']:\n","        if pd.notnull(known_album) and known_album.lower() in query.lower():\n","            album = known_album\n","            break\n","\n","    for known_year in data['year']:\n","        if str(known_year) in query:\n","            year = known_year\n","            break\n","\n","    return artist, album, year"],"metadata":{"id":"UZnrdTTG4IKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","k_values = [3, 6, 10]\n","top_k = 10\n","all_ngram_evaluations = []\n","\n","for n in range(1, 6):\n","    evaluation_results = []\n","\n","    for idx, row in ground_truth_df.iterrows():\n","        query = row['Processed_Query']\n","        relevant_ids = set(row['Relevant_Song_IDs'])\n","\n","        artist, album, year = parse_query_for_graph(query, lyrics_df)\n","\n","        tfidf_retrieved_docs = retrieve_with_tfidf(\n","            lyrics_df, query, ngram_range=(n, n), top_k=top_k\n","        )\n","\n","        for doc in tfidf_retrieved_docs:\n","            doc_id = doc['id']\n","\n","            if doc_id in lyrics_df['no'].values:\n","                lyric_row = lyrics_df.loc[lyrics_df['no'] == doc_id]\n","                artist_name = lyric_row['artist'].values[0] if not lyric_row['artist'].isnull().all() else None\n","                album_name = lyric_row['album'].values[0] if not lyric_row['album'].isnull().all() else None\n","\n","                if artist_name:\n","                    doc['artist_score'] = get_artist_graph_score(artist_name, kg, query)\n","                else:\n","                    print(f\"Warning: artist_name is invalid for doc_id {doc_id}\")\n","                    doc['artist_score'] = 0\n","\n","                if album_name:\n","                    doc['album_score'] = get_album_graph_score(album_name, kg, query)\n","                else:\n","                    print(f\"Warning: album_name is invalid for doc_id {doc_id}\")\n","                    doc['album_score'] = 0\n","\n","                doc['combined_score'] = (\n","                    doc.get('tfidf_score', 0)\n","                    + doc.get('artist_score', 0)\n","                    + doc.get('album_score', 0)\n","                )\n","            else:\n","                print(f\"Warning: doc_id {doc_id} not found in lyrics_df\")\n","\n","        combined_retrieved_docs = sorted(\n","            tfidf_retrieved_docs, key=lambda x: x['combined_score'], reverse=True\n","        )[:top_k]\n","        combined_retrieved_ids = [doc['id'] for doc in combined_retrieved_docs]\n","\n","        evaluation = evaluate_query(relevant_ids, combined_retrieved_ids, k_values)\n","        evaluation['Query'] = row['Processed_Query']\n","        evaluation['N-gram'] = n\n","        evaluation_results.append(evaluation)\n","\n","    eval_df = pd.DataFrame(evaluation_results)\n","    print(f\"\\nEvaluation Results for {n}-gram:\")\n","    print(eval_df.head())\n","\n","    macro_results = {\n","        \"Metric\": [\n","            \"Precision@3\", \"Recall@3\", \"F1@3\",\n","            \"Precision@6\", \"Recall@6\", \"F1@6\",\n","            \"Precision@10\", \"Recall@10\", \"F1@10\",\n","            \"Average Precision\"\n","        ],\n","        \"Macro Average\": [\n","            eval_df['Precision@3'].mean(),\n","            eval_df['Recall@3'].mean(),\n","            eval_df['F1@3'].mean(),\n","            eval_df['Precision@6'].mean(),\n","            eval_df['Recall@6'].mean(),\n","            eval_df['F1@6'].mean(),\n","            eval_df['Precision@10'].mean(),\n","            eval_df['Recall@10'].mean(),\n","            eval_df['F1@10'].mean(),\n","            eval_df['Average Precision'].mean()\n","        ]\n","    }\n","    macro_df = pd.DataFrame(macro_results)\n","    macro_df['N-gram'] = n\n","    print(f\"\\nMacro Averages for {n}-gram:\")\n","    print(macro_df)\n","\n","    eval_df.to_excel(f'evaluation_combined_complex_tfidf_graph_{n}gram.xlsx', index=False)\n","    macro_df.to_excel(f'macro_averages_combined_complex_graph_{n}gram.xlsx', index=False)\n","\n","    all_ngram_evaluations.append(macro_df)\n","\n","combined_macro = pd.concat(all_ngram_evaluations, ignore_index=True)\n","combined_macro.to_excel('combined_macro_averages_complex_all_ngrams_with_graph.xlsx', index=False)\n","print(\"\\nAll macro averages for all n-grams have been saved to 'combined_macro_averages_all_ngrams_with_graph.xlsx'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72EoWd6Zh1TR","executionInfo":{"status":"ok","timestamp":1734563108117,"user_tz":-420,"elapsed":1044470,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"b2fbf3a6-0498-42fe-8057-ce088e9dfd2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluation Results for 1-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6      F1@6  Precision@10  \\\n","0     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","1     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","2     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","3     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","4     0.000000       0.0   0.0     0.166667       1.0  0.285714           0.1   \n","\n","   Recall@10     F1@10  Average Precision                       Query  N-gram  \n","0        0.0  0.000000           0.000000               adel love you       1  \n","1        1.0  0.181818           0.333333   billi eilish stick togeth       1  \n","2        0.0  0.000000           0.000000          bruno mar kiss you       1  \n","3        0.0  0.000000           0.000000  jennif lopez you feel left       1  \n","4        1.0  0.181818           0.166667             hopeless romant       1  \n","\n","Macro Averages for 1-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.080000       1\n","1           Recall@3       0.180000       1\n","2               F1@3       0.102857       1\n","3        Precision@6       0.066667       1\n","4           Recall@6       0.273333       1\n","5               F1@6       0.102032       1\n","6       Precision@10       0.048000       1\n","7          Recall@10       0.333333       1\n","8              F1@10       0.081219       1\n","9  Average Precision       0.172492       1\n","\n","Evaluation Results for 2-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6      F1@6  Precision@10  \\\n","0     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","1     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","2     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","3     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","4     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","\n","   Recall@10     F1@10  Average Precision                       Query  N-gram  \n","0        0.0  0.000000                0.0               adel love you       2  \n","1        1.0  0.181818                1.0   billi eilish stick togeth       2  \n","2        0.0  0.000000                0.0          bruno mar kiss you       2  \n","3        0.0  0.000000                0.0  jennif lopez you feel left       2  \n","4        1.0  0.181818                0.5             hopeless romant       2  \n","\n","Macro Averages for 2-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.160000       2\n","1           Recall@3       0.433333       2\n","2               F1@3       0.229333       2\n","3        Precision@6       0.086667       2\n","4           Recall@6       0.473333       2\n","5               F1@6       0.144603       2\n","6       Precision@10       0.056000       2\n","7          Recall@10       0.486667       2\n","8              F1@10       0.098974       2\n","9  Average Precision       0.386000       2\n","\n","Evaluation Results for 3-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6      F1@6  Precision@10  \\\n","0     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","1     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","2     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","3     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","4     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","\n","   Recall@10     F1@10  Average Precision                       Query  N-gram  \n","0        0.0  0.000000                0.0               adel love you       3  \n","1        0.0  0.000000                0.0   billi eilish stick togeth       3  \n","2        0.0  0.000000                0.0          bruno mar kiss you       3  \n","3        1.0  0.181818                1.0  jennif lopez you feel left       3  \n","4        0.0  0.000000                0.0             hopeless romant       3  \n","\n","Macro Averages for 3-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.173333       3\n","1           Recall@3       0.480000       3\n","2               F1@3       0.252000       3\n","3        Precision@6       0.093333       3\n","4           Recall@6       0.493333       3\n","5               F1@6       0.154603       3\n","6       Precision@10       0.064000       3\n","7          Recall@10       0.520000       3\n","8              F1@10       0.111795       3\n","9  Average Precision       0.450921       3\n","\n","Evaluation Results for 4-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision                       Query  N-gram  \n","0        0.0    0.0                0.0               adel love you       4  \n","1        0.0    0.0                0.0   billi eilish stick togeth       4  \n","2        0.0    0.0                0.0          bruno mar kiss you       4  \n","3        0.0    0.0                0.0  jennif lopez you feel left       4  \n","4        0.0    0.0                0.0             hopeless romant       4  \n","\n","Macro Averages for 4-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.080000       4\n","1           Recall@3       0.240000       4\n","2               F1@3       0.120000       4\n","3        Precision@6       0.040000       4\n","4           Recall@6       0.240000       4\n","5               F1@6       0.068571       4\n","6       Precision@10       0.024000       4\n","7          Recall@10       0.240000       4\n","8              F1@10       0.043636       4\n","9  Average Precision       0.220000       4\n","\n","Evaluation Results for 5-gram:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6  F1@6  Precision@10  \\\n","0          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","1          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","2          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","3          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","4          0.0       0.0   0.0          0.0       0.0   0.0           0.0   \n","\n","   Recall@10  F1@10  Average Precision                       Query  N-gram  \n","0        0.0    0.0                0.0               adel love you       5  \n","1        0.0    0.0                0.0   billi eilish stick togeth       5  \n","2        0.0    0.0                0.0          bruno mar kiss you       5  \n","3        0.0    0.0                0.0  jennif lopez you feel left       5  \n","4        0.0    0.0                0.0             hopeless romant       5  \n","\n","Macro Averages for 5-gram:\n","              Metric  Macro Average  N-gram\n","0        Precision@3       0.040000       5\n","1           Recall@3       0.120000       5\n","2               F1@3       0.060000       5\n","3        Precision@6       0.020000       5\n","4           Recall@6       0.120000       5\n","5               F1@6       0.034286       5\n","6       Precision@10       0.012000       5\n","7          Recall@10       0.120000       5\n","8              F1@10       0.021818       5\n","9  Average Precision       0.120000       5\n","\n","All macro averages for all n-grams have been saved to 'combined_macro_averages_all_ngrams_with_graph.xlsx'.\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def rrf_fusion(ranked_lists, rrf_k=60):\n","    \"\"\"\n","    Perform Reciprocal Rank Fusion (RRF) on multiple ranked lists.\n","\n","    Parameters:\n","    - ranked_lists: List of lists, where each sublist contains document IDs ordered by relevance.\n","    - rrf_k: Constant to control the influence of rank.\n","\n","    Returns:\n","    - fused_docs: List of document IDs ordered by their RRF scores.\n","    \"\"\"\n","    rrf_scores = defaultdict(float)\n","    for ranked_list in ranked_lists:\n","        for rank, doc in enumerate(ranked_list):\n","            if isinstance(doc, dict):\n","                doc_id = doc.get('id')\n","            else:\n","                doc_id = doc\n","            rrf_scores[doc_id] += 1.0 / (rrf_k + rank + 1)\n","    fused_ranking = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n","    fused_docs = [doc_id for doc_id, score in fused_ranking]\n","    return fused_docs\n","\n","\n","rrf_evaluations = []\n","\n","for idx, row in ground_truth_df.iterrows():\n","    query = row['Processed_Query']\n","    relevant_ids = set(row['Relevant_Song_IDs'])\n","\n","    ranked_lists = []\n","    for n in range(1, 3):\n","        retrieved_docs = retrieve_with_tfidf(lyrics_df, query, ngram_range=(n,n), top_k=top_k)\n","        ranked_lists.append(retrieved_docs)\n","\n","    fused_docs = rrf_fusion(ranked_lists, rrf_k=60)\n","\n","    rrf_eval = evaluate_query(relevant_ids, fused_docs, k_values)\n","    rrf_eval['Query'] = row['query']\n","    rrf_evaluations.append(rrf_eval)\n","\n","rrf_df = pd.DataFrame(rrf_evaluations)\n","print(\"\\nRRF Fused Evaluation Results:\")\n","print(rrf_df.head())\n","\n","rrf_macro_results = {\n","    \"Metric\": [\n","        \"Precision\",\n","        \"Recall\",\n","        \"F1\",\n","        \"Precision@3\",\n","        \"Precision@6\",\n","        \"Precision@10\",\n","        \"Average Precision\"\n","    ],\n","    \"Macro Average\": [\n","        rrf_df['Precision@10'].mean(),\n","        rrf_df['Recall@10'].mean(),\n","        rrf_df['F1@10'].mean(),\n","        rrf_df['Precision@3'].mean(),\n","        rrf_df['Precision@6'].mean(),\n","        rrf_df['Precision@10'].mean(),\n","        rrf_df['Average Precision'].mean()\n","    ]\n","}\n","rrf_macro_df = pd.DataFrame(rrf_macro_results)\n","rrf_macro_df['N-gram'] = 'RRF Fusion'\n","print(\"\\nMacro Averages for RRF Fusion:\")\n","print(rrf_macro_df)\n","\n","rrf_df.to_excel('evaluation_tfidf_rrf_fusion.xlsx', index=False)\n","rrf_macro_df.to_excel('macro_averages_rrf_fusion.xlsx', index=False)\n","print(\"\\nRRF Fused evaluation results have been saved to 'evaluation_tfidf_rrf_fusion.xlsx'.\")\n","print(\"Macro averages for RRF Fusion have been saved to 'macro_averages_rrf_fusion.xlsx'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoF3vB9_6-R8","executionInfo":{"status":"ok","timestamp":1734564646362,"user_tz":-420,"elapsed":186548,"user":{"displayName":"nadila fitri","userId":"05612191379251951150"}},"outputId":"b1bb845d-6380-4b1c-be6b-ef776ccb55ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","RRF Fused Evaluation Results:\n","   Precision@3  Recall@3  F1@3  Precision@6  Recall@6      F1@6  Precision@10  \\\n","0     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","1     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","2     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","3     0.000000       0.0   0.0     0.000000       0.0  0.000000           0.0   \n","4     0.333333       1.0   0.5     0.166667       1.0  0.285714           0.1   \n","\n","   Recall@10     F1@10  Average Precision  \\\n","0        0.0  0.000000                0.0   \n","1        1.0  0.181818                0.5   \n","2        0.0  0.000000                0.0   \n","3        0.0  0.000000                0.0   \n","4        1.0  0.181818                0.5   \n","\n","                                               Query  \n","0               Adele's song with lyrics love you in  \n","1    Billie Eilish's song with lyrics stick together  \n","2              Bruno Mars' song with lyrics kiss you  \n","3    Jennifer Lopez's song with lyrics you feel left  \n","4  Song released in 2024 with lyrics hopeless rom...  \n","\n","Macro Averages for RRF Fusion:\n","              Metric  Macro Average      N-gram\n","0          Precision       0.060000  RRF Fusion\n","1             Recall       0.473333  RRF Fusion\n","2                 F1       0.103643  RRF Fusion\n","3        Precision@3       0.133333  RRF Fusion\n","4        Precision@6       0.066667  RRF Fusion\n","5       Precision@10       0.060000  RRF Fusion\n","6  Average Precision       0.224585  RRF Fusion\n","\n","RRF Fused evaluation results have been saved to 'evaluation_tfidf_rrf_fusion.xlsx'.\n","Macro averages for RRF Fusion have been saved to 'macro_averages_rrf_fusion.xlsx'.\n"]}]}]}